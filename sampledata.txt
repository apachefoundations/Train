#1
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

x = np.array([1, 2, 3, 4, 5], dtype=float)
y = 2 * x + 1

model = keras.Sequential([
    layers.Dense(units=1, input_shape=[1])
])

model.compile(optimizer='sgd', loss='mean_squared_error')

model.fit(x, y, epochs=100, verbose=0)

test_input = np.array([6.0])
prediction = model.predict(test_input)

print("Prediction for input 6.0:", prediction[0][0])

#2
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [0], [0], [1]])

model = keras.Sequential([
    layers.Dense(units=1, input_shape=(2,), activation='binary_crossentropy')
])

model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X, y, epochs=100, verbose=0)

predictions = model.predict(X)
print("Predictions:\n", predictions.round())

#6
import numpy as np
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)

param_dist = {
    "max_depth": [3, None],
    "max_features": randint(1, 9),
    "min_samples_leaf": randint(1, 9),
    "criterion": ["gini", "entropy"]
}

tree = DecisionTreeClassifier()
search = RandomizedSearchCV(tree, param_dist, cv=5, n_iter=10, random_state=42)
search.fit(X, y)

print("Best Parameters:", search.best_params_)
print("Best Score:", search.best_score_)

#3
import random
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

SEED_VALUE = 42
random.seed(SEED_VALUE)
np.random.seed(SEED_VALUE)
tf.random.set_seed(SEED_VALUE)

(X_train_all, y_train_all), (X_test, y_test) = mnist.load_data()

X_valid = X_train_all[:10000]
X_train = X_train_all[10000:]
y_valid = y_train_all[:10000]
y_train = y_train_all[10000:]

plt.figure(figsize=(18, 5))
for i in range(3):
    plt.subplot(1, 3, i + 1)
    plt.axis('off')
    plt.imshow(X_train[i], cmap='gray')
plt.show()

X_train = X_train.reshape((X_train.shape[0], 28 * 28)).astype("float32") / 255
X_valid = X_valid.reshape((X_valid.shape[0], 28 * 28)).astype("float32") / 255
X_test = X_test.reshape((X_test.shape[0], 28 * 28)).astype("float32") / 255

y_train = to_categorical(y_train)
y_valid = to_categorical(y_valid)
y_test = to_categorical(y_test)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train,
          epochs=10,
          batch_size=32,
          validation_data=(X_valid, y_valid))

test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

#5
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

train_images = train_images / 255.0
test_images = test_images / 255.0

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

plt.figure(figsize=(9, 9))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(train_images[i])
    plt.title(class_names[train_labels[i][0]])
    plt.axis('off')
plt.show()

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=10,
                    validation_data=(test_images, test_labels))

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f"Test Accuracy: {test_acc:.2f}")


#10
import numpy as np  
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense

data = np.random.rand(1000, 10, 1)

timesteps = 10
input_dim = 1
latent_dim = 4

inputs = Input(shape=(timesteps, input_dim))
encoded = LSTM(latent_dim)(inputs)

decoded = RepeatVector(timesteps)(encoded)
decoded = LSTM(latent_dim, return_sequences=True)(decoded)
decoded = TimeDistributed(Dense(1))(decoded)

autoencoder = Model(inputs, decoded)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()

autoencoder.fit(data, data, epochs=50, batch_size=32, validation_split=0.2)

#15

import numpy as np

num_states = 10
num_actions = 10
Q = np.zeros((num_states, num_actions))

alpha = 0.1
gamma = 0.9
epsilon = 0.1

def simulate_environment(state, action):
    reward = np.random.choice([1, 0], p=[0.2, 0.8])
    next_state = (state + action) % num_states
    return next_state, reward

def train_q_learning(num_episodes=1000):
    for episode in range(num_episodes):
        state = np.random.randint(0, num_states)
        for _ in range(num_states):
            if np.random.uniform(0, 1) < epsilon:
                action = np.random.randint(0, num_actions)
            else:
                action = np.argmax(Q[state, :])

            next_state, reward = simulate_environment(state, action)

            Q[state, action] = (1 - alpha) * Q[state, action] + \
                alpha * (reward + gamma * np.max(Q[next_state, :]))

            state = next_state

def generate_response(state):
    action = np.argmax(Q[state, :])
    return f"Response {action} for context {state}"

def interactive_dialogue():
    print("Welcome to the dialogue system!")
    while True:
        try:
            context = int(input("Enter a dialogue context (0-9), or -1 to quit: "))
            if context == -1:
                break
            if 0 <= context < num_states:
                print(generate_response(context))
            else:
                print("Invalid context. Try a number between 0 and 9.")
        except ValueError:
            print("Please enter a valid number.")

train_q_learning()
interactive_dialogue()

#14
import numpy as np
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt

(X_digits, y_digits), _ = tf.keras.datasets.mnist.load_data()
X_digits = np.expand_dims(X_digits, axis=-1).astype(np.float32) / 255.0

grid_size = 16

def make_sample_image(X, y):
    for _ in range(3):
        idx = np.random.randint(len(X_digits))
        digit = X_digits[idx] @ (np.random.rand(1, 3) + 0.1)
        label = y_digits[idx]
        px, py = np.random.randint(0, 100), np.random.randint(0, 100)
        mx, my = (px + 14) // grid_size, (py + 14) // grid_size

        channels = y[my][mx]
        if channels[0] > 0:
            continue

        channels[0] = 1.0
        channels[1] = px - (mx * grid_size)
        channels[2] = py - (my * grid_size)
        channels[3] = 28.0
        channels[4] = 28.0
        channels[5 + label] = 1.0

        X[py:py + 28, px:px + 28] += digit

def make_data(size=1):
    X = np.zeros((size, 128, 128, 3), dtype=np.float32)
    y = np.zeros((size, 8, 8, 15), dtype=np.float32)
    for i in range(size):
        make_sample_image(X[i], y[i])
    return np.clip(X, 0.0, 1.0), y

def get_color(prob):
    return (1., 0., 0.) if prob < 0.3 else (1., 1., 0.) if prob < 0.7 else (0., 1., 0.)

def show_predictions(image, labels, threshold=0.1):
    img = image.copy()
    for my in range(8):
        for mx in range(8):
            ch = labels[my][mx]
            prob, x1, y1, w, h = ch[:5]
            if prob < threshold:
                continue
            color = get_color(prob)
            px, py = int(mx * grid_size + x1), int(my * grid_size + y1)
            cv2.rectangle(img, (px, py), (px + int(w), py + int(h)), color, 1)
            label = np.argmax(ch[5:])
            cv2.rectangle(img, (px, py - 10), (px + 12, py), color, -1)
            cv2.putText(img, f'{label}', (px + 2, py - 2),
                        cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 0, 0))
    plt.imshow(img)
    plt.axis(False)

X, y = make_data(1)
show_predictions(X[0], y[0])
plt.show()

#7
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator

class_names = ['Cats', 'Dogs']

train_dir = r'C:\Users\LENOVO\PycharmProjects\nn\train'
train_gen = ImageDataGenerator(rescale=1./255)
train_data = train_gen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(len(class_names), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_data, epochs=5)

model.save(r'C:\Users\LENOVO\PycharmProjects\nn\resnet50_model.h5')

img_path = r'C:\Users\LENOVO\PycharmProjects\nn\pet.jpg'
img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))
img_array = tf.keras.preprocessing.image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0) / 255.0

pred = model.predict(img_array)
predicted_class = class_names[np.argmax(pred)]

plt.imshow(img)
plt.title(f"Predicted: {predicted_class}")
plt.axis('off')
plt.show()

#9
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
import numpy as np

vocab_size = 10000
maxlen = 500
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)

model = Sequential([
    Embedding(vocab_size, 32),
    SimpleRNN(32),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))

word_index = imdb.get_word_index()
index_word = {v + 3: k for k, v in word_index.items()}
index_word[0] = "<PAD>"
index_word[1] = "<START>"
index_word[2] = "<UNK>"
index_word[3] = "<UNUSED>"

def review_encode(text):
    encoded = [1]
    for word in text.lower().split():
        if word in word_index and word_index[word] + 3 < vocab_size:
            encoded.append(word_index[word] + 3)
        else:
            encoded.append(2)
    return pad_sequences([encoded], maxlen=maxlen)

while True:
    review = input("Enter a movie review (or type 'exit' to quit): ")
    if review.lower() == "exit":
        break
    encoded = review_encode(review)
    prediction = model.predict(encoded)[0][0]
    sentiment = "Positive" if prediction > 0.5 else "Negative"
    print(f"Predicted sentiment: {sentiment} (confidence: {prediction:.2f})\n")

#11
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models

(X_train, _), _ = tf.keras.datasets.mnist.load_data()
X_train = (X_train.astype(np.float32) - 127.5) / 127.5
X_train = X_train.reshape(-1, 28 * 28)

latent_dim = 100
batch_size = 128
epochs = 10000
sample_interval = 1000

def build_generator():
    model = models.Sequential([
        layers.Dense(128, activation='relu', input_dim=latent_dim),
        layers.Dense(784, activation='tanh')
    ])
    return model

def build_discriminator():
    model = models.Sequential([
        layers.Dense(128, activation='relu', input_dim=784),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

optimizer = tf.keras.optimizers.Adam(0.0002)

discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

generator = build_generator()
z = layers.Input(shape=(latent_dim,))
img = generator(z)
discriminator.trainable = False
valid = discriminator(img)
combined = models.Model(z, valid)
combined.compile(loss='binary_crossentropy', optimizer=optimizer)

for epoch in range(1, epochs + 1):
    idx = np.random.randint(0, X_train.shape[0], batch_size)
    real_imgs = X_train[idx]
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    fake_imgs = generator.predict(noise)
    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))
    d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))

    if epoch % 100 == 0:
        print(f"{epoch} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")

    if epoch % sample_interval == 0:
        r, c = 5, 5
        noise = np.random.normal(0, 1, (r * c, latent_dim))
        gen_imgs = generator.predict(noise)
        gen_imgs = 0.5 * gen_imgs + 0.5
        gen_imgs = gen_imgs.reshape(-1, 28, 28)
        fig, axs = plt.subplots(r, c, figsize=(5, 5))
        for i in range(r * c):
            axs[i // c, i % c].imshow(gen_imgs[i], cmap='gray')
            axs[i // c, i % c].axis('off')
        plt.show()
#12
import tensorflow as tf
import numpy as np
from PIL import Image

model = tf.keras.applications.VGG16(weights="imagenet")
img_path = "example.jpg"

img = Image.open(img_path).resize((224, 224))
img_array = tf.keras.preprocessing.image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = tf.keras.applications.vgg16.preprocess_input(img_array)

preds = model.predict(img_array)
decoded = tf.keras.applications.vgg16.decode_predictions(preds, top=3)[0]

for label in decoded:
    print(f"{label[1]}: {label[2]*100:.2f}%")

#13
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, Model

data = {
    'user_id': [0, 0, 1, 1, 2, 2],
    'item_id': [0, 1, 1, 2, 0, 2],
    'rating':  [5, 3, 4, 2, 1, 5]
}
df = pd.DataFrame(data)

num_users = df.user_id.nunique()
num_items = df.item_id.nunique()

X = df[['user_id', 'item_id']].values
y = df['rating'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

class Recommender(Model):
    def __init__(self, num_users, num_items, embedding_size=10):
        super().__init__()
        self.user_embedding = layers.Embedding(num_users, embedding_size)
        self.item_embedding = layers.Embedding(num_items, embedding_size)
        self.dot = layers.Dot(axes=1)
        self.dense = layers.Dense(1)

    def call(self, inputs):
        user_vec = self.user_embedding(inputs[:, 0])
        item_vec = self.item_embedding(inputs[:, 1])
        x = self.dot([user_vec, item_vec])
        return self.dense(x)

model = Recommender(num_users, num_items)
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=100, verbose=0)

preds = model.predict(X_test)
for i in range(len(X_test)):
    print(f"User {X_test[i][0]}, Item {X_test[i][1]} -> Predicted Rating: {preds[i][0]:.2f}")







